[
  {
    "id": "1",
    "slug": "flask-rest-api-setup",
    "title": " Flask REST API Setup üß™",
    "subtitle": "A Step-by-Step Tutorial",
    "author": "Keith Thomson",
    "content": "## A Step-by-Step Tutorial\n\n---\n\n## üìã Introduction\n\nIn this tutorial, we'll walk through setting up a **basic Flask REST API**. We'll cover the essential steps, from installing dependencies to creating routes for **CRUD operations**.\n\n---\n\n## üõ†Ô∏è Step 1: Install Dependencies\n\nTo start, you'll need to install **Flask** and **Flask-RESTful**. Use `pip` to install them:\n\n```bash\npip install flask flask-restful\n```\n\n### üìÇ Step 2: Create a New Flask App\nCreate a new file called app.py and add the following code:\n```python\nfrom flask import Flask\nfrom flask_restful import Api\n\napp = Flask(__name__)\napi = Api(app)\n\n@app.route('/')\ndef home():\n    return \"Welcome to my API!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nThis sets up a basic Flask app with a single route.\n\n### üîß Step 3: Define Your API Endpoints\nLet's create a simple API for managing books. We'll define endpoints for CRUD operations:\nfrom flask_restful import Resource, reqparse\n\n# Sample in-memory data store\n```python\nbooks = [\n    {\"id\": 1, \"title\": \"Book 1\", \"author\": \"Author 1\"},\n    {\"id\": 2, \"title\": \"Book 2\", \"author\": \"Author 2\"}\n]\n\nclass BookList(Resource):\n    def get(self):\n        return books\n\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str, required=True)\n        parser.add_argument(\"author\", type=str, required=True)\n        args = parser.parse_args()\n        new_book = {\n            \"id\": len(books) + 1,\n            \"title\": args[\"title\"],\n            \"author\": args[\"author\"]\n        }\n        books.append(new_book)\n        return new_book, 201\n\nclass Book(Resource):\n    def get(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        return book\n\n    def put(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str)\n        parser.add_argument(\"author\", type=str)\n        args = parser.parse_args()\n        book[\"title\"] = args.get(\"title\", book[\"title\"])\n        book[\"author\"] = args.get(\"author\", book[\"author\"])\n        return book\n\n    def delete(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        books.remove(book)\n        return {\"message\": \"Book deleted\"}\n\napi.add_resource(BookList, \"/books\")\napi.add_resource(Book, \"/books/<int\\:book_id>\")\n```\n\n## Endpoint Summary\n- BookListGET/booksRetrieve all books\n- BookListPOST/booksCreate a new book\n- BookGET/books/<book_id>Retrieve a single book\n- BookPUT/books/<book_id>Update a book\n- BookDELETE/books/<book_id>Delete a book\n\n### ‚ñ∂Ô∏è Step 4: Run Your API\nRun your API using:\npython app.py\nYou can now interact with your API using tools like curl or a REST client.\n\n### üìå Example Use Cases\nGet all bookscurl http://localhost:5000/books \nCreate a new book \n```bash \ncurl -X POST -H \"Content-Type: application/json\" -d '{\"title\": \"New Book\", \"author\": \"New Author\"}' http://localhost:5000/booksGet a single bookcurl http://localhost:5000/books/1Update a bookcurl -X PUT -H \"Content-Type: application/json\" -d '{\"title\": \"Updated Book\"}' http://localhost:5000/books/1Delete a bookcurl -X DELETE http://localhost:5000/books/1\n```\n\n## üéØ Conclusion\nThis tutorial provides a basic setup for a Flask REST API. You can build upon this example to create more complex APIs with additional features like:\n ",
    "summary": "Learn how to set up a Flask REST API from scratch with simple code examples.",
    "read_time": "8 min read",
    "tags": "flask,rest,api,python,webdev",
    "category": "flask",
    "created_on": "2025-04-12 13:12:06",
    "updated_on": "2025-07-04 12:20:25",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "12",
    "slug": "python-regex",
    "title": "Regular Expressions for Python",
    "subtitle": "Harnessing the Power of Regex",
    "author": "Keith Thomson",
    "content": "## ![Python Logo](https://www.python.org/static/community_logos/python-powered-w-200x80.png)  üîç  Mastering Regular Expressions: A Comprehensive Guide\n\n## Introduction\n\nRegular expressions (regex) are a **üî• powerful tool** for **pattern matching** and **text manipulation**. They allow you to **üîç search, üìù extract, and üîÑ replace** specific patterns within strings, making them invaluable for tasks like:\n- **‚úÖ Data validation**\n- **üìä Parsing**\n- **üîé Text mining**\n- **üìÇ Log analysis**\n- **üîÑ Search-and-replace operations**\n\nThis guide will introduce you to the **fundamental concepts, syntax, and real-world applications** of regular expressions.\n\n---\n\n## üìã Table of Contents\n1. [Basic Syntax](#basic-syntax)\n2. [Special Characters](#special-characters)\n3. [Grouping and Capturing](#grouping-and-capturing)\n4. [Lookaheads and Lookbehinds](#lookaheads-and-lookbehinds)\n5. [Common Use Cases](#common-use-cases)\n6. [Regex in Python](#regex-in-python)\n7. [Performance Considerations](#performance-considerations)\n8. [Practical Examples](#practical-examples)\n9. [Debugging and Testing](#debugging-and-testing)\n10. [Conclusion](#conclusion)\n\n ### üÖ∞Ô∏è Character Classes  \nMatch **sets of characters**:  \n\n - `[abc]` Matches **a**, **b**, or **c**.  \n - `[a-z]` Matches any **lowercase letter**.  \n - `[A-Z]` Matches any **uppercase letter**.  \n - `[0-9]` Matches any **digit**.  \n - `[a-zA-Z0-9]` Matches any **alphanumeric** character.  \n - `[^a-z]` The `^` inside brackets **negates the set**, matching any character **except** a lowercase letter.  \n\n---\n\n### üèÅ Anchors  \nMatch a **position** within the string, not a character:  \n\n - `^` Matches the **beginning** of the string. ‚Äî `^Hello` matches `\"Hello world\"`.  \n - `$` Matches the **end** of the string. ‚Äî `world$` matches `\"Hello world\"`.  \n - `\b` Matches a **word boundary** (between a word and non-word character). ‚Äî `\bcat\b` matches `\"cat\"` but not `\"caterpillar\"`.  \n - `\\B` Matches a **non-word boundary**. ‚Äî `\\Bcat\\B` matches `\"caterpillar\"` but not `\"cat\"`.  \n\n---\n\n## ‚ö° Special Character Metacharacters  \nThese are **shorthand** for common character classes.  \n\n| `.` | Matches **any single character** (except newline). \n\n | `a.c` | matches | `\"abc\"`, `\"a1c\"` |\n\n| `\\d` | Matches any **digit**. Equivalent to `[0-9]`. \n\n | `\\d` matches `1`, `9` |\n| `\\D` \n\n | Matches any **non-digit**. Equivalent to `[^0-9]`. | \n\n `\\D` matches `a`, `@` |\n\n| `\\w` | Matches any **word** character (alphanumeric + underscore). | \n\n `\\w` matches `a`, `_`, `1` |\n\n| `\\W` | Matches any **non-word** character. | `\\W` matches `#`, `!` |\n| `\\s` | Matches any **whitespace** (space, tab, newline). | \n\n `\\s` matches `\" \"`, `\t` |\n\n| `\\S` | Matches any **non-whitespace**. | \n\n `\\S` matches `a`, `1` |\n\n | `.` | Matches **any single character** (except newline `\n`). \n\n | `a.c`| matches `\"abc\"`, `\"a1c\"` |\n\n| `\\d` | Matches any **digit**. Equivalent to `[0-9]`. | `\\d` matches `1`, `9` |\n\n| `\\D` | Matches any **non-digit**. Equivalent to `[^0-9]`. | `\\D` matches `a`, `@` |\n\n| `\\w` | Matches any **word** character (alphanumeric + underscore). | `\\w` matches `a`, `_`, `1` |\n| `\\W` | Matches any **non-word** character. | `\\W` matches `#`, `!` |\n| `\\s` | Matches any **whitespace** (space, tab, newline). | `\\s` matches `\" \"`, `\t` |\n| `\\S` | Matches any **non-whitespace**. | `\\S` matches `a`, `1` |\n\n | `[a-z]`      | Matches any lowercase letter.| `a`, `b`, `z`|\n\n | `[A-Z]` | Matches any uppercase letter. | `A`, `B`, `Z` | \n\n | `[0-9]` | Matches any digit. | `0`, `1`, `9` | \n\n | `[a-zA-Z0-9]`| Matches any alphanumeric character. | `a`, `B`, `1` | \n\n | `[^a-z]` | Matches any character **except** lowercase letters. | `A`, `1`, `@` |\n\n ### üè∑Ô∏è Anchors\nMatch the **beginning or end** of a string:\n\n | `^`    | Matches the **beginning** of the string. \n\n `^hello` matches `\"hello world\"` \n\n | `$`    | Matches the **end** of the string. \n\n `world$` matches `\"hello world\"` |\n\n---\n\n### üî¢ Quantifiers\nSpecify **how many times** a character or group should be repeated:\n | Syntax  | Description                                      | Example                     |\n |---------|--------------------------------------------------|-----------------------------|\n | `*`     | Matches **zero or more** occurrences.             | `a*` matches `\"\"`, `\"a\"`, `\"aa\"` |\n | `+`     | Matches **one or more** occurrences.              | `a+` matches `\"a\"`, `\"aa\"`  |\n | `?`     | Matches **zero or one** occurrence.               | `a?` matches `\"\"`, `\"a\"`    |\n | `{n}`   | Matches **exactly n** occurrences.                | `a{3}` matches `\"aaa\"`      |\n | `{n,}`  | Matches **n or more** occurrences.                | `a{2,}` matches `\"aa\"`, `\"aaa\"` |\n | `{n,m}` | Matches **between n and m** occurrences.          | `a{2,4}` matches `\"aa\"`, `\"aaa\"`, `\"aaaa\"` |\n\n---\n\n## ‚ö° Special Characters \n | Syntax | Description                                      | Example                     |\n |--------|--------------------------------------------------|-----------------------------|\n | `.`    | Matches **any character** (except newline).      | `a.c` matches `\"abc\"`, `\"a1c\"` |\n | `\\d`   | Matches a **digit**.                             | `\\d` matches `1`, `2`       |\n | `\\w`   | Matches a **word character**.                    | `\\w` matches `a`, `_`, `1`  |\n | `\\s`   | Matches **whitespace**.                          | `\\s` matches `\" \"`, `\\t`    |\n | `\\D`   | Matches a **non-digit** character.               | `\\D` matches `a`, `@`       |\n | `\\W`   | Matches a **non-word** character.                | `\\W` matches `@`, `#`       |\n | `\\S`   | Matches a **non-whitespace** character.          | `\\S` matches `a`, `1`       |\n\n---\n\n## ü§ù Grouping and Capturing \n\nParentheses `()` are used to **group** parts of a regex and **capture** matched text for extraction or backreferencing.\n | Syntax       | Description                                      | Example                     |\n |--------------|--------------------------------------------------|-----------------------------|\n | `(pattern)`  | Groups the pattern.                              | `(abc)`                     |\n | `\\1`, `\\2`   | Refer to captured groups (**backreferences**).   | `(a).\\1` matches `\"aba\"`    |\n\n**Example:**\nTo extract the **area code** and **phone number** from a string like `\"(123) 456-7890\"`:\n\n```regex\n$(\\d{3})$ (\\d{3}-\\d{4})\nPython Example:\nimport re\n\ntext = \"(123) 456-7890\"\npattern = r\"$(\\d{3})$ (\\d{3}-\\d{4})\"\nmatch = re.search(pattern, text)\n\nif match:\n    area_code = match.group(1)  # \"123\"\n    phone_number = match.group(2)  # \"456-7890\"\n    print(f\"üìû Area Code: {area_code}, Phone: {phone_number}\")\n```\n\n## üí° Common Use Cases \n| Type | Syntax |\n|------|--------|\n|‚úâÔ∏è Email Validation | ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\$ |\n| üåê Extracting URLs | https?://[^\\s]+ |\n| üìÖ Finding Dates | \\d{2}-\\d{2}-\\d{4} |\n| üîí Password Strength Check | ^(?=.*[A-Z])(?=.*[a-z])(?=.*\\d)(?=.*[@\\$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$ |\n---\n* Validates email addresses (e.g., \"user@example.com\").\n* Matches HTTP/HTTPS URLs in text.\n* Matches dates in DD-MM-YYYY format.\n* Ensures passwords have at least one uppercase letter, one lowercase letter, one digit, one special character, and are at least 8 characters long.\n\n\n## üêç **Regex in Python** \nPython‚Äôs **re** module provides full support for regular expressions:\nimport re\n\n## üîç Search for a pattern\n```python\ntext = \"The quick brown fox jumps over the lazy dog.\"\nmatch = re.search(r\"brown \\w+\", text)\nprint(match.group())  # \"brown fox\"\n```\n\n## üìã Find all occurrences\n```python\nmatches = re.findall(r\"\\b\\w{3}\\b\", text)\nprint(matches)  # ['The', 'fox', 'the', 'dog']\n```\n\n### üîÑ Replace Text\n```python\nnew_text = re.sub(r\"fox\", \"cat\", text)\nprint(new_text)  # \"The quick brown cat jumps over the lazy dog.\"\n```\n\n---\n\n## ‚ö° Performance Considerations\n\n- ‚ö†Ô∏è Avoid greedy quantifiers (e.g., `.*`) when possible. Use non-greedy quantifiers (e.g., `.*?`) for efficiency.\n- üöÄ Pre-compile regex patterns for repeated use:\n  ```python\n  pattern = re.compile(r\"\\d{3}-\\d{4}\")\n  ```\n- Use specific patterns instead of generic ones (e.g., `\\d` instead of `.`).\n\n---\n\n## üìÇ Practical Examples {#Practical Examples} \n\n### 1. üè∑Ô∏è Extracting Hashtags\n```python\ntext = \"Love #regex! It's #awesome for #text processing.\"\nhashtags = re.findall(r\"#\\w+\", text)\nprint(hashtags)  # ['#regex', '#awesome', '#text']\n```\n\n### 2. üìú Parsing Log Files\n```python\nlog_entry = '127.0.0.1 - james [01/Jan/2025:12:34:56 +0000] \"GET /index.html\" 200 1234'\npattern = r'(\\S+) - (\\S+)$$\n(.*?)\n$$ \"(\\S+ \\S+)\" (\\d+) (\\d+)'\nmatch = re.search(pattern, log_entry)\nif match:\n    ip, user, date, request, status, size = match.groups()\n    print(f\"üñ•Ô∏è IP: {ip}, üë§ User: {user}, üìÑ Request: {request}\")\n```\n\n### 3. üìû Validating Phone Numbers\n```python\nphone_pattern = r'^(\\+\\d{1,3}[- ]?)?\\d{10}\\$'\nprint(re.match(phone_pattern, \"+1-1234567890\"))  # ‚úÖ Valid\nprint(re.match(phone_pattern, \"12345\"))  # ‚ùå Invalid\n```\n\n---\n\n## üõ†Ô∏è Debugging and Testing \n- Use online tools like [Regex101](https://regex101.com/) to test and debug regex patterns.\n- Break complex patterns into smaller, manageable parts.\n\n---\n\n## üìä Regex Cheat Sheet\n![Regex Cheat Sheet](https://i.imgur.com/OQStwMn.png)\nCredit: *https://i.imgur.com/OQStwMn.png*\n\n---\n\n\n## üéØ Conclusion \n\nRegular expressions are a versatile and powerful tool for text processing. By mastering the syntax and applying best practices, you can efficiently solve a wide range of string manipulation tasks. Start with simple patterns, gradually build complexity, and always test your regex against real-world data.\n\n**Next Steps:**\n- Practice with real-world datasets.\n- Explore regex in other programming languages (e.g., JavaScript, Perl).\n- Learn advanced techniques like recursive patterns and conditional matching.\n- Learn advanced techniques like recursive patterns and conditional matching.",
    "summary": "Master regular expressions in Python with practical examples, syntax breakdowns, and performance tips.",
    "read_time": "4 min read",
    "tags": "regex",
    "category": "regex",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "13",
    "slug": "iot-sensors-future",
    "title": "IoT Sensors ",
    "subtitle": "The Future of Intelligent Sensing - A Quick Intro into Next-Gen IoT Sensors",
    "author": "Keith Thomson",
    "content": "## üåç Overview of IoT Sensors\n\nBy **2030**, it‚Äôs projected that there will be over **100 billion connected devices** around the globe. Behind every **smart thermostat**, **wearable health tracker**, or **autonomous drone** lies one essential technology: **sensors**.\n\nAs the **core enablers of the Internet of Things (IoT)**, sensors are evolving from **basic signal collectors** into **intelligent, edge-processing nodes** that can understand and act on the world around them.\n\nThis article explores:\n- The transformation of sensors into **intelligent agents**.\n- The impact on **industries, privacy, and connected ecosystems**.\n\n---\n\n## üìà The Evolution of IoT Sensors\n\nEarlier generations of sensors simply collected **analog or digital data** (e.g., temperature, motion, voltage) and relayed it to a central server. Today‚Äôs IoT sensors are **smarter**, thanks to:\n   Feature                     | Description                                                                 |\n |-----------------------------|-----------------------------------------------------------------------------|\n | **Microcontrollers**        | Embedded directly within the sensor for local processing.                  |\n | **On-device AI**            | Models that interpret data at the edge, reducing cloud dependency.         |\n | **Power-efficient protocols** | BLE, LoRa, Zigbee for low-energy communication.                            |\n | **Energy harvesting**       | Solar, thermal, or vibration-based power to extend battery life.           |\n\nThis evolution turns sensors into **autonomous decision-makers**, not just data providers.\n\n---\n\n## üå± Real-World Use Case: Agriculture\n\nIn **smart farming**, soil sensors measure **moisture, salinity, and temperature** to optimize irrigation schedules. Modern sensors can:\n- Apply **threshold logic** or **anomaly detection** on-site.\n- Trigger **actuators or alerts** without human input.\n\n### Example: DHT22 Temperature & Humidity Sensor on Raspberry Pi\n\n```python\nimport Adafruit_DHT\n\nsensor = Adafruit_DHT.DHT22\npin = 4\n\nhumidity, temp = Adafruit_DHT.read_retry(sensor, pin)\nif humidity is not None and temp is not None:\n    print(f\"Temperature: {temp:.1f}¬∞C | Humidity: {humidity:.1f}%\")\nelse:\n    print(\"Sensor read error\")\n```\n\n## üèôÔ∏è Applications Across Industries\n\nSmart Cities Air quality monitoring, smart streetlights, and traffic sensors. HealthcareWearables that detect heart rate variability and stress levels. Industrial IoTSensors monitor machinery vibrations to predict failures. Environmental Forest fire early detection using heat and smoke sensors.\n\n## ‚ö†Ô∏è Challenges Ahead\nInteroperability Many vendors, few standards.Security Edge devices can be attack vectors.Data Privacy Always-on sensors raise surveillance concerns.\n\n## üîÆ Conclusion\n\nThe future of IoT sensors lies not just in miniaturization, but in autonomy. With edge AI and smarter hardware, the next wave of sensors won‚Äôt just measure the world‚Äîthey‚Äôll respond to it.\nFrom greener cities to efficient factories and intuitive homes, smart sensors are laying the foundation for a truly connected future.\n\n---\n\n### Key Improvements:\n1. **Added emojis and icons** for visual appeal.\n2. **Used tables** for structured information.\n3. **Fixed code block syntax** for Python example.\n4. **Improved readability** with clear sections and bullet points.\n5. **Removed line breaks** within paragraphs for cleaner Markdown.\n6. **Added a title** for better context.",
    "summary": "An exploration of next-gen IoT sensors, their architecture, and how they‚Äôre shaping intelligent sensing systems.",
    "read_time": "5 min read",
    "tags": "raspberry pi,sensors,iot,edge computing",
    "category": "IoT",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "14",
    "slug": "using-finance-apis",
    "title": "Using Finance APIs to Build Smart Financial Tools",
    "subtitle": "Using Finance APIs to Build Smart Financial Tools",
    "author": "Keith Thomson",
    "content": "## Introduction\n\nIn the age of real-time analytics, Finance APIs have become the cornerstone of modern financial applications. Whether you're building a stock tracking dashboard, a personal budgeting app, or an algorithmic trading bot, these APIs offer developers an open window into financial markets. With a few lines of code, you can query live stock prices, historical charts, company financials, and even macroeconomic indicators.\n\nThis guide explores how Finance APIs work, how to integrate them, and what to consider when building secure, scalable financial applications.\n\n## Understanding the Landscape\n\nAPIs like **Alpha Vantage**, **Finnhub**, **IEX Cloud**, and **Yahoo Finance (via RapidAPI)** provide access to a wide range of data:\n\n- **Equity prices (real-time and historical)**\n- **Financial statements (balance sheets, income, cash flow)**\n- **Forex and cryptocurrency rates**\n- **Economic indicators like GDP, CPI, and interest rates**\n\nMany offer a free tier ‚Äî suitable for prototyping ‚Äî but have limits on calls per minute or daily quotas.\n\n## Choosing the Right API\n\n- **Alpha Vantage**: Best for free historical data and technical indicators.\n- **Finnhub**: Great for global markets and alternative datasets like news sentiment.\n- **IEX Cloud**: US-focused, reliable for intraday data.\n- **Yahoo Finance**: Wide coverage but often requires a third-party wrapper.\n\nWhen choosing an API, consider:\n\n- Update frequency (real-time vs delayed)\n- API stability and response time\n- Licensing restrictions (especially for commercial use)\n\n## Sample Integration: Alpha Vantage in Python\n\nHere's a basic example of how to retrieve and print live stock prices:\n\npython\nimport requests\n\nAPI_KEY = \\\"your_api_key\\\"\nsymbol = \\\"AAPL\\\"\n\nurl = f\\\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey={API_KEY}\\\"\nresponse = requests.get(url)\ndata = response.json()\n\nprice = data[\\\"Global Quote\\\"][\\\"05. price\\\"]\nvolume = data[\\\"Global Quote\\\"][\\\"06. volume\\\"]\nprint(f\\\"{symbol} Price: ${price} | Volume: {volume}\\\")\n\nBuilding Smarter Tools\n\nWith real-time data, you can create:\n\n Alert systems for price thresholds\n\n Charts showing moving averages and RSI\n\n Personalized portfolios with tracked holdings\n\n Trading bots (carefully regulated)\n\nAlways cache or debounce frequent calls to stay within limits, and design UI/UX with delay tolerance if you're using free plans with slower response times.\nSecurity and Privacy\n\nWhen handling finance data:\n\n Secure your API keys in environment variables\n\n Validate all responses and handle exceptions gracefully\n\n Never expose user portfolio data without encryption and authorization\n\nConclusion\n\nFinance APIs unlock powerful capabilities for developers at any level. From hobbyist dashboards to fintech platforms, they bridge the gap between global markets and modern software. The key is understanding your data, handling it responsibly, and building for performance and scale.",
    "summary": "Learn how to use Finance APIs like yFinance, Alpha Vantage, and Finnhub to build smart financial tools.",
    "read_time": "6 min read",
    "tags": "finance,api,stocks,data,fintech",
    "category": "stocks",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "15",
    "slug": "ethics-2050",
    "title": "Ethics for The Future",
    "subtitle": "Big Data, Smart Homes, Surveillance and Tracking... ",
    "author": "Keith Thomson",
    "content": "As technology accelerates, the ethical challenges humanity faces grow in complexity and scope. The future will not be defined solely by innovation, but by **how responsibly** we manage data, surveillance, and connected environments.\n\n## üìä Big Data and Human Autonomy\n\nBig Data is often described as the ‚Äúnew oil,‚Äù but unlike oil, data comes from individuals‚Äîour actions, communications, and even our biology. While analytics drive medical breakthroughs and economic efficiencies, they also pose risks:\n\n - **Healthcare analytics improve early disease detection**\n  - Ethical Risk: Loss of privacy from sensitive genetic data\n- **Predictive policing helps identify crime hotspots**\n  - Ethical Risk: Reinforcement of bias and discrimination\n- **Targeted advertising provides personalization**\n  - Ethical Risk: Manipulation of consumer behavior.\n\n> ‚ÄúThe measure of intelligence is the ability to change.‚Äù ‚Äî Albert Einstein\n\nThe ethical question is whether individuals truly consent to how their data is used. In most cases, data is gathered passively, raising doubts about informed consent.\n\n## üè† Smart Homes: Comfort vs. Control\n\nSmart devices promise convenience‚Äîthermostats learn our schedules, fridges track groceries, and voice assistants answer questions instantly. But this convenience comes at a cost: **constant data collection**.\n\nImagine a smart home ecosystem:\n\n- Motion sensors track movement patterns\n- Cameras monitor visitors\n- Energy systems report daily routines\n\nThis data, if leaked or misused, creates risks ranging from burglary to corporate exploitation. The ethical responsibility lies in designing systems that **prioritize user autonomy and minimize surveillance creep**.\n\n## üëÅÔ∏è Surveillance and Tracking\n\nSurveillance has shifted from CCTV cameras on street corners to digital tracking embedded in smartphones, wearables, and cars. While such tracking aids navigation and public safety, it raises profound ethical dilemmas:\n\n1. **Transparency**: Are users aware of what is collected?\n2. **Proportionality**: Does the level of surveillance match the threat?\n3. **Accountability**: Who safeguards misuse of this data?\n\nA growing concern is the emergence of **social credit systems**, where individuals‚Äô behavior is monitored, scored, and judged. Such models blur the line between governance and authoritarianism.\n\n## üåç Toward an Ethical Future\n\nTo balance innovation with human dignity, several guiding principles are essential:\n\n- **Privacy by design**: Systems should minimize data collection by default.\n- **Right to be forgotten**: Individuals must control digital footprints.\n- **Equity and fairness**: Algorithms should be audited for bias.\n- **Transparency**: Companies must disclose practices clearly.\n\n## üîÆ Conclusion\n\nThe future of humanity will be shaped by how we govern the tension between **progress and ethics**. Big Data, smart homes, and pervasive surveillance hold both promise and peril. If left unchecked, these technologies could erode freedoms and autonomy. But if guided by strong ethical frameworks, they can empower individuals, strengthen societies, and safeguard human dignity.\n\n> ‚ÄúTechnology is a useful servant but a dangerous master.‚Äù ‚Äî Christian Lous Lange\n",
    "summary": "Explore the ethical implications of big data, smart homes, and surveillance technologies in shaping the future.",
    "read_time": "7 min read",
    "tags": "ethics,IoT,digital governance",
    "category": "ethics",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "19",
    "slug": "getting-started-with-rust",
    "title": "Getting Started with Rust",
    "subtitle": "A comprehensive guide to learning Rust programming language",
    "author": "Keith Thomson",
    "content": "# üöÄ Getting Started \n\nRust is a **systems programming language** that runs blazingly fast, prevents segfaults, and guarantees thread safety. In this post, we‚Äôll explore the **fundamentals of Rust**, why it‚Äôs becoming increasingly popular among developers, and walk through practical examples that show how Rust differs from other languages.  \n\n\n\n## üåü Why Rust?  \n\nRust offers several advantages over traditional systems programming languages like **C** and **C++**:  \n\n- **Memory Safety**: Prevents common bugs like null pointer dereferences and buffer overflows.  \n- **Performance**: Zero-cost abstractions ‚Äî you don‚Äôt pay for features you don‚Äôt use.  \n- **Concurrency**: Built-in support for safe, data-race-free concurrent programming.  \n- **Modern Ecosystem**: A strong package manager (`cargo`), vibrant community, and modern tooling.  \n\n> üí° **Tip:** Rust enforces correctness at compile time, saving you from runtime surprises that are common in C/C++.  \n\n---\n\n## üî§ Basic Concepts  \n\nLet‚Äôs start with the classic **Hello, World!** program:  \n\n```rust\nfn main() {\n    println!(\"Hello, World!\");\n}\n```\n\nThis simple program demonstrates Rust‚Äôs **clean syntax** and its **macro system** (notice the `!` in `println!`). Macros in Rust are more powerful than standard functions ‚Äî they can generate code at compile time.  \n\n---\n\n## üìù Variables and Mutability  \n\nIn Rust, variables are **immutable by default**. This means once you assign a value, it cannot change unless you explicitly declare it as mutable.  \n\n```rust\nfn main() {\n    let x = 5;        // immutable\n    let mut y = 10;   // mutable\n\n    println!(\"x = {}\", x);\n    println!(\"y = {}\", y);\n\n    y = 15;\n    println!(\"y (after change) = {}\", y);\n}\n```\n\n- `let` creates a variable.  \n- `mut` makes it mutable.  \n- Rust encourages immutability to reduce bugs and improve safety.  \n\n---\n\n## üîë Ownership and Borrowing  \n\nRust‚Äôs **ownership system** is its most unique feature. It enforces memory safety without a garbage collector.  \n\n### Example: Ownership  \n\n```rust\nfn main() {\n    let s1 = String::from(\"Rust\");\n    let s2 = s1; // ownership moved from s1 to s2\n\n    // println!(\"{}\", s1); // ‚ùå Error: s1 is no longer valid\n    println!(\"{}\", s2);   // ‚úÖ Works\n}\n```\n\n- Variables own their data.  \n- When ownership is transferred (moved), the old variable is invalidated.  \n\n### Example: Borrowing  \n\n```rust\nfn main() {\n    let s = String::from(\"Borrowing in Rust\");\n    print_length(&s); // pass reference (borrow)\n    println!(\"s is still valid: {}\", s);\n}\n\nfn print_length(s: &String) {\n    println!(\"Length: {}\", s.len());\n}\n```\n\n- `&` means ‚Äúborrow without taking ownership.‚Äù  \n- The original variable remains valid.  \n\n> üîí This system prevents dangling pointers and memory leaks at compile time.  \n\n---\n\n## üîß Functions and Control Flow  \n\nRust functions look familiar, but with strong typing and return value rules.  \n\n```rust\nfn main() {\n    println!(\"Sum = {}\", add(5, 7));\n\n    let number = 6;\n    if number % 2 == 0 {\n        println!(\"Even\");\n    } else {\n        println!(\"Odd\");\n    }\n}\n\nfn add(a: i32, b: i32) -> i32 {\n    a + b  // no semicolon = return value\n}\n```\n\n- Functions must declare parameter and return types.  \n- Leaving out the semicolon `;` makes it an **expression** that returns a value.  \n\n---\n\n## ‚ö†Ô∏è Error Handling  \n\nRust does not have exceptions. Instead, it uses:  \n- `Result<T, E>` for recoverable errors.  \n- `Option<T>` for values that may or may not exist.  \n\n```rust\nuse std::fs::File;\nuse std::io::ErrorKind;\n\nfn main() {\n    let file = File::open(\"data.txt\");\n\n    match file {\n        Ok(_) => println!(\"File opened successfully.\"),\n        Err(ref e) if e.kind() == ErrorKind::NotFound => {\n            println!(\"File not found, creating one...\");\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n        }\n    }\n}\n```\n\nThis forces you to **handle errors explicitly**.  \n\n---\n\n## üßµ Concurrency  \n\nRust makes concurrency safe by design. Threads must follow ownership and borrowing rules.  \n\n```rust\nuse std::thread;\n\nfn main() {\n    let handles: Vec<_> = (1..5).map(|i| {\n        thread::spawn(move || {\n            println!(\"Hello from thread {}\", i);\n        })\n    }).collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n}\n```\n\n- `move` transfers ownership into the thread.  \n- No data races are possible because Rust enforces safe access at compile time.  \n\n---\n\n## üèÅ Conclusion  \n\nRust combines **the performance of C/C++** with **modern safety guarantees** and a thriving ecosystem. Its ownership model may take time to learn, but it pays off by eliminating entire classes of bugs.  \n\nWhether you‚Äôre building:  \n- üöÄ **High-performance applications**  \n- üåê **Web servers with frameworks like Actix or Axum**  \n- üìä **Data pipelines and concurrent systems**  \n- üîí **Secure, low-level embedded software**  \n\nRust is quickly becoming the **go-to language for systems programming** in the modern era.  \n\n---\n\nüí° *Next Step:* Try rewriting a small project you‚Äôve built in Python, Go, or C into Rust ‚Äî you‚Äôll immediately see how ownership, borrowing, and safety rules shape your design.  ",
    "summary": "Learn the basics of Rust programming language, from installation to writing your first program.",
    "read_time": "8 min read \n",
    "tags": "rust,programming,tutorial",
    "category": "rust",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "20",
    "slug": "building-web-apps-with-axum",
    "title": "Building Web Applications with Axum",
    "subtitle": "Modern web development using Rust and the Axum framework",
    "author": "Keith Thomson",
    "content": "# ‚ö° Let's get started!  \n\nAxum is a **web application framework** that focuses on ergonomics and modularity. Built on top of **Tokio** (for async runtime) and **Tower** (for middleware and services), it provides a solid foundation for building **scalable, fast, and reliable web applications** in Rust.  \n\nIn this post, we‚Äôll walk through setting up an Axum project, creating routes, handling requests, adding middleware, and building APIs with JSON.  \n\n\n## üîß Setting Up Your Project  \n\nFirst, let‚Äôs create a new Rust project and add Axum as a dependency:  \n\n```bash\ncargo new my-web-app\ncd my-web-app\ncargo add axum tokio --features tokio/full\ncargo add tower-http --features full\ncargo add serde serde_json --features derive\n```  \n\nThis will set up a new project with **Axum**, **Tokio**, **Tower HTTP utilities**, and **Serde** for JSON support.  \n\n---\n\n## üåç Creating Your First Route  \n\nHere‚Äôs how to create a simple HTTP server with Axum:  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello, World!\" }));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis creates a basic web server that responds with `\"Hello, World!\"` on the root path.  \n\nRun it with:  \n\n```bash\ncargo run\n```  \n\nVisit [http://localhost:3000](http://localhost:3000) in your browser to see it in action.  \n\n---\n\n## üîó Handling Path and Query Parameters  \n\nAxum makes it easy to capture path and query parameters.  \n\n```rust\nuse axum::{extract::Path, routing::get, Router};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/hello/:name\", get(greet));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn greet(Path(name): Path<String>) -> String {\n    format!(\"Hello, {}!\", name)\n}\n```\n\nNow, visiting `/hello/Alice` will return:  \n\n```\nHello, Alice!\n```  \n\nYou can also extract query parameters using `axum::extract::Query`.  \n\n---\n\n## üì¶ Returning JSON Responses  \n\nAxum integrates with `serde` for JSON serialization.  \n\n```rust\nuse axum::{routing::get, Json, Router};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct Message {\n    message: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/json\", get(get_message));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn get_message() -> Json<Message> {\n    Json(Message {\n        message: \"Hello from JSON!\".to_string(),\n    })\n}\n```\n\nVisiting `/json` will return:  \n\n```json\n{\"message\": \"Hello from JSON!\"}\n```  \n\n---\n\n## üõ° Adding Middleware  \n\nAxum builds on **Tower**, so you can add middleware like logging, timeouts, or request limits.  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\nuse tower_http::trace::TraceLayer;\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello with middleware!\" }))\n        .layer(TraceLayer::new_for_http()); // log requests\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis logs each request/response, useful for debugging and monitoring.  \n\n---\n\n## üî® Building a Small REST API  \n\nHere‚Äôs a simple **in-memory todo API** with Axum:  \n\n```rust\nuse axum::{\n    extract::{Path, State},\n    routing::{get, post},\n    Json, Router,\n};\nuse serde::{Deserialize, Serialize};\nuse std::sync::{Arc, Mutex};\n\n#[derive(Serialize, Deserialize, Clone)]\nstruct Todo {\n    id: usize,\n    text: String,\n}\n\n#[derive(Clone, Default)]\nstruct AppState {\n    todos: Arc<Mutex<Vec<Todo>>>,\n}\n\n#[tokio::main]\nasync fn main() {\n    let state = AppState::default();\n\n    let app = Router::new()\n        .route(\"/todos\", get(list_todos).post(add_todo))\n        .route(\"/todos/:id\", get(get_todo))\n        .with_state(state);\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn list_todos(State(state): State<AppState>) -> Json<Vec<Todo>> {\n    let todos = state.todos.lock().unwrap().clone();\n    Json(todos)\n}\n\nasync fn add_todo(State(state): State<AppState>, Json(todo): Json<Todo>) -> Json<Todo> {\n    let mut todos = state.todos.lock().unwrap();\n    todos.push(todo.clone());\n    Json(todo)\n}\n\nasync fn get_todo(Path(id): Path<usize>, State(state): State<AppState>) -> Option<Json<Todo>> {\n    let todos = state.todos.lock().unwrap();\n    todos.iter().find(|t| t.id == id).cloned().map(Json)\n}\n```\n\nEndpoints:  \n- `GET /todos` ‚Üí List todos  \n- `POST /todos` ‚Üí Add a todo (JSON body)  \n- `GET /todos/:id` ‚Üí Fetch a todo by ID  \n\n---\n\n## üèÅ Conclusion  \n\nAxum provides:  \n- Clean, ergonomic APIs for routing and request handling.  \n- Native async support via Tokio.  \n- Integration with Tower for middleware.  \n- Strong type safety and Rust‚Äôs memory guarantees.  \n\nIt‚Äôs an excellent choice for building **web servers, REST APIs, and microservices** in Rust.  \n\nüí° *Next Step:* Extend this project with persistent storage (SQLite, Postgres, or Redis) to turn it into a production-ready API.  ",
    "summary": "Learn how to build modern web applications using Rust and the Axum framework.",
    "read_time": "12 min read",
    "tags": "rust,webdev,http",
    "category": "rust",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "21",
    "slug": "async-rust-programming",
    "title": "Mastering Async Programming in Rust",
    "subtitle": "Understanding futures, async/await, and concurrent programming patterns",
    "author": "Keith Thomson",
    "content": "# ‚è≥ Mastering Asynchronous Programming in Rust  \n\nAsynchronous programming is one of **Rust's most powerful features**, enabling you to write **highly concurrent and performant applications**. This guide will walk you through the fundamentals of async Rust, covering futures, the async/await syntax, and practical examples using the Tokio runtime.  \n\n\n## üîÆ Understanding Futures  \n\nIn Rust, asynchronous operations are represented by **futures**. A future is a value that may not be available yet, but will be at some point in the future.  \n\nA future does nothing on its own until it is **polled** by an executor (like Tokio).  \n\n```rust\nuse std::future::Future;\n\nfn example_future() -> impl Future<Output = i32> {\n    async {\n        42\n    }\n}\n```\n\nHere, the future resolves to `42` once awaited.  \n\n---\n\n## üìù The async/await Syntax  \n\nThe `async` keyword turns a function into a future, and `await` is used to wait for that future to complete:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn fetch_data() -> String {\n    // Simulate async work\n    sleep(Duration::from_secs(1)).await;\n    \"Data fetched!\".to_string()\n}\n\n#[tokio::main]\nasync fn main() {\n    let result = fetch_data().await;\n    println!(\"{}\", result);\n}\n```\n\nThis **non-blocking approach** allows your application to handle thousands of concurrent operations efficiently.  \n\n---\n\n## üßµ Spawning Tasks with Tokio  \n\nTokio provides an async runtime for executing tasks concurrently. You can spawn lightweight async tasks using `tokio::spawn`:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn task(id: i32) {\n    println!(\"Task {} started\", id);\n    sleep(Duration::from_secs(2)).await;\n    println!(\"Task {} finished\", id);\n}\n\n#[tokio::main]\nasync fn main() {\n    let handle1 = tokio::spawn(task(1));\n    let handle2 = tokio::spawn(task(2));\n\n    // Wait for both tasks to finish\n    let _ = tokio::join!(handle1, handle2);\n}\n```\n\nOutput (order may vary):  \n```\nTask 1 started\nTask 2 started\nTask 1 finished\nTask 2 finished\n```\n\n---\n\n## üìÇ Using async with I/O  \n\nAsync is especially powerful when handling **I/O-bound tasks** like networking or file access.  \n\nExample: Reading from TCP using async:  \n\n```rust\nuse tokio::net::TcpListener;\nuse tokio::io::{AsyncReadExt, AsyncWriteExt};\n\n#[tokio::main]\nasync fn main() -> tokio::io::Result<()> {\n    let listener = TcpListener::bind(\"127.0.0.1:8080\").await?;\n\n    loop {\n        let (mut socket, _) = listener.accept().await?;\n\n        tokio::spawn(async move {\n            let mut buf = [0; 1024];\n            let n = socket.read(&mut buf).await.unwrap();\n\n            if n > 0 {\n                socket.write_all(&buf[0..n]).await.unwrap();\n            }\n        });\n    }\n}\n```\n\nThis creates a simple **async echo server**.  \n\n---\n\n## ‚ö†Ô∏è Error Handling in Async  \n\nYou can use `Result` and the `?` operator with async functions just like synchronous code:  \n\n```rust\nuse tokio::fs::File;\nuse tokio::io::{self, AsyncReadExt};\n\nasync fn read_file(path: &str) -> io::Result<String> {\n    let mut file = File::open(path).await?;\n    let mut contents = String::new();\n    file.read_to_string(&mut contents).await?;\n    Ok(contents)\n}\n\n#[tokio::main]\nasync fn main() -> io::Result<()> {\n    match read_file(\"example.txt\").await {\n        Ok(data) => println!(\"File contents: {}\", data),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n    Ok(())\n}\n```\n\n---\n\n## üî® Building a Simple Async App  \n\nLet‚Äôs combine everything into a **mini async downloader**:  \n\n```rust\nuse reqwest;\nuse tokio;\n\nasync fn fetch_url(url: &str) -> reqwest::Result<String> {\n    let response = reqwest::get(url).await?;\n    let body = response.text().await?;\n    Ok(body)\n}\n\n#[tokio::main]\nasync fn main() {\n    let url = \"https://www.rust-lang.org\";\n    match fetch_url(url).await {\n        Ok(html) => println!(\"Downloaded {} bytes\", html.len()),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n}\n```\n\nThis example fetches the HTML of Rust‚Äôs official site asynchronously.  \n\n---\n\n## üèÅ Conclusion  \n\nAsync Rust lets you:  \n- Handle **thousands of concurrent tasks** efficiently.  \n- Write **non-blocking I/O operations** with Tokio.  \n- Use familiar patterns like `async/await`, `Result`, and `?`.  \n- Build **high-performance servers** and **networked apps**.  \n\nüí° *Next Step:* Try combining Axum and async Rust to build a full-featured web API ‚Äî you‚Äôll see the true power of Rust‚Äôs async ecosystem.  ",
    "summary": "Master asynchronous programming in Rust with futures, async/await, and concurrent patterns.",
    "read_time": "15 min read \n",
    "tags": "rust",
    "category": "rust,programming",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "22",
    "slug": "go-data-structures",
    "title": "Data Structures in Go: A Comprehensive Guide",
    "subtitle": "Exploring arrays, slices, maps, structs, and more in Go",
    "author": "Keith Thomson",
    "content": "### üêπ **Go** provides a rich set of built-in data structures that make it powerful for both systems programming and application development. \n\nIn this guide, we‚Äôll explore the core data structures available in Go, explain how they work, and show practical code examples.  \n\n\n## üî¢ Arrays  \n\nAn **array** in Go is a fixed-size, ordered collection of elements.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var arr [3]int = [3]int{1, 2, 3}\n    fmt.Println(arr)\n\n    // Iterate\n    for i, v := range arr {\n        fmt.Printf(\"Index %d = %d\n\", i, v)\n    }\n}\n```\n\n- Arrays have a fixed length.  \n- Useful when the size is known and constant.  \n\n\n\n## üìê Slices  \n\nA **slice** is a dynamically-sized, flexible view into an array. Slices are the most commonly used data structure in Go.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    slice := []int{1, 2, 3, 4, 5}\n    slice = append(slice, 6)\n    fmt.Println(slice)\n    fmt.Println(\"Length:\", len(slice), \"Capacity:\", cap(slice))\n}\n```\n\n* Built on top of arrays.  \n* Support dynamic resizing with `append`.  \n* Preferred over arrays in most cases.  \n\n\n\n## üó∫ Maps  \n\nA **map** is Go‚Äôs built-in hash table implementation for key-value pairs.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    m := map[string]int{\n        \"Alice\": 25,\n        \"Bob\":   30,\n    }\n    m[\"Charlie\"] = 35\n\n    for k, v := range m {\n        fmt.Printf(\"%s is %d years old\n\", k, v)\n    }\n}\n```\n\n- Keys must be comparable (e.g., strings, ints).  \n- Lookups are O(1) on average.  \n\n\n\n## üèó Structs  \n\nA **struct** groups fields together, making it Go‚Äôs way of creating custom data types.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Person struct {\n    Name string\n    Age  int\n}\n\nfunc main() {\n    p := Person{Name: \"Alice\", Age: 30}\n    fmt.Println(p.Name, \"is\", p.Age)\n}\n```\n\n- Structs are used for modeling entities.  \n- They are the building blocks of more complex data structures.  \n\n\n## üîó Linked Lists  \n\nGo‚Äôs standard library provides a **doubly linked list** via `container/list`.  \n\n```go\npackage main\n\nimport (\n    \"container/list\"\n    \"fmt\"\n)\n\nfunc main() {\n    l := list.New()\n    l.PushBack(1)\n    l.PushBack(2)\n    l.PushFront(0)\n\n    for e := l.Front(); e != nil; e = e.Next() {\n        fmt.Println(e.Value)\n    }\n}\n```\n\n- Each element points to the next and previous nodes.  \n- Efficient for insertions/removals in the middle.  \n\n\n\n## üìö Stacks  \n\nA **stack** is a LIFO (Last In, First Out) structure. Implemented easily with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Stack []int\n\nfunc (s *Stack) Push(v int) {\n    *s = append(*s, v)\n}\n\nfunc (s *Stack) Pop() int {\n    if len(*s) == 0 {\n        panic(\"stack is empty\")\n    }\n    val := (*s)[len(*s)-1]\n    *s = (*s)[:len(*s)-1]\n    return val\n}\n\nfunc main() {\n    var s Stack\n    s.Push(10)\n    s.Push(20)\n    fmt.Println(s.Pop()) // 20\n}\n```\n\n- Built using slices.  \n- Great for recursion-like problems, parsing, and backtracking.  \n\n---\n\n## üì¨ Queues  \n\nA **queue** is a FIFO (First In, First Out) structure. Also implemented with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Queue []int\n\nfunc (q *Queue) Enqueue(v int) {\n    *q = append(*q, v)\n}\n\nfunc (q *Queue) Dequeue() int {\n    if len(*q) == 0 {\n        panic(\"queue is empty\")\n    }\n    val := (*q)[0]\n    *q = (*q)[1:]\n    return val\n}\n\nfunc main() {\n    var q Queue\n    q.Enqueue(1)\n    q.Enqueue(2)\n    fmt.Println(q.Dequeue()) // 1\n}\n```\n\n- Useful for scheduling and breadth-first search (BFS).  \n\n\n\n## ‚õ∞ Heaps & Priority Queues  \n\nGo provides heap operations in the `container/heap` package.  \n\n```go\npackage main\n\nimport (\n    \"container/heap\"\n    \"fmt\"\n)\n\ntype IntHeap []int\n\nfunc (h IntHeap) Len() int           { return len(h) }\nfunc (h IntHeap) Less(i, j int) bool { return h[i] < h[j] }\nfunc (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\n\nfunc (h *IntHeap) Push(x any) {\n    *h = append(*h, x.(int))\n}\n\nfunc (h *IntHeap) Pop() any {\n    old := *h\n    n := len(old)\n    x := old[n-1]\n    *h = old[0 : n-1]\n    return x\n}\n\nfunc main() {\n    h := &IntHeap{3, 1, 4}\n    heap.Init(h)\n    heap.Push(h, 2)\n    fmt.Println(heap.Pop(h)) // 1 (smallest element)\n}\n```\n\n- Implements a min-heap by default.  \n- Can be adapted into a priority queue.  \n\n---\n\n## üèÅ Conclusion  \n\nGo provides both **high-level abstractions** (slices, maps, structs) and **low-level control** (linked lists, heaps).  \nBy mastering these data structures, you‚Äôll be ready to build efficient algorithms, design scalable applications, and handle both systems and business logic effectively.  \n\nüí° *Next Step:* Try implementing algorithms like BFS, DFS, and Dijkstra‚Äôs algorithm using these data structures to strengthen your understanding.",
    "summary": "A deep dive into Go‚Äôs built-in data structures with examples.",
    "read_time": "15 min read",
    "tags": "go,data structures,programming",
    "category": "golang",
    "created_on": "2025-08-23 14:24:56",
    "updated_on": "2025-08-23 14:24:56",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "23",
    "slug": "google-big-query-python",
    "title": "Using Google BigQuery with Python: A Full Guide",
    "subtitle": "A Google BigQuery Tutorial ",
    "author": "Keith Thomson",
    "content": "## A Practical Guide \n\n![](https://locusit.com/wp-content/uploads/2024/12/Google-BigQuery.jpeg)\n\nGoogle BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. When combined with Python üêç, it becomes a powerful tool for data engineers, analysts, and scientists.\n\nThis guide provides **real-world code examples** and best practices for integrating BigQuery with Python on Google Cloud Platform (GCP).\n\n---\n![](https://www.python.org/static/community_logos/python-logo-master-v3-TM.png)\n\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Setting Up Authentication](#setting-up-authentication)\n3. [Connecting to BigQuery](#connecting-to-bigquery)\n4. [Querying Data from BigQuery](#querying-data-from-bigquery)\n5. [Loading Data into BigQuery](#loading-data-into-bigquery)\n6. [Writing Data to BigQuery](#writing-data-to-bigquery)\n7. [Scheduled Queries with Python](#scheduled-queries-with-python)\n8. [Optimizing Query Performance](#optimizing-query-performance)\n9. [Exporting Data from BigQuery](#exporting-data-from-bigquery)\n10. [Error Handling and Logging](#error-handling-and-logging)\n11. [Cost Management](#cost-management)\n12. [Advanced Use Cases](#advanced-use-cases)\n13. [Integrating with Other GCP Services](#integrating-with-other-gcp-services)\n14. [Security Best Practices](#security-best-practices)\n15. [Conclusion](#conclusion)\n\n---\n\n## Prerequisites \n\nBefore you begin, ensure you have the following:\n- A **Google Cloud Platform (GCP) account** with billing enabled.\n- A **GCP project** with the BigQuery API enabled.\n- **Python 3.7+** installed on your local machine or cloud environment.\n- The **Google Cloud SDK** installed and authenticated:\n  ```bash\n  gcloud auth application-default login\n\nThe **google-cloud-bigquery** and **pandas** libraries installed:\npip install google-cloud-bigquery pandas\n\n\n\n__Setting Up Authentication__ <a name=\"setting-up-authentication\"></a>\nTo interact with BigQuery from Python, you need to authenticate using a service account:\n\n\n## Create a Service Account in GCP:\n\n__Create a new service account__ and assign it the BigQuery Admin role. Navigate to IAM & Admin > Service Accounts.\nGenerate a JSON key file and download it.\n\n\n\n__Set the Environment Variable:__\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/service-account-key.json\"\n\n\n\n__Connecting to BigQuery__ <a name=\"connecting-to-bigquery\"></a>\nUse the google-cloud-bigquery library to establish a connection:\nfrom google.cloud import bigquery\n\n## Initialize a BigQuery client\n```python\nclient = bigquery.Client()\n```\n## Querying Data from BigQuery \n__Example:__  Analyzing E-Commerce Sales Data\nSuppose you have a dataset containing e-commerce transactions. You want to analyze daily sales trends:\n```python\ndef query_daily_sales():\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales,\n            COUNT(DISTINCT user_id) AS unique_customers\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)  # Run the query\n    results = query_job.result()  # Wait for the query to complete\n\n    for row in results:\n        print(f\"Date: {row.transaction_date}, Sales: \\${row.total_sales}, Customers: {row.unique_customers}\")\n\nquery_daily_sales()\n```\n### Key Points:\n\nUse parameterized queries to avoid SQL injection.\nFor large datasets, use query_job.to_dataframe() to convert results to a Pandas DataFrame for further analysis.\n\n__Example:__  Parameterized Queries\n```python\ndef query_sales_by_date(start_date, end_date):\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) BETWEEN @start_date AND @end_date\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    job_config = bigquery.QueryJobConfig(\n        query_parameters=[\n            bigquery.ScalarQueryParameter(\"start_date\", \"DATE\", start_date),\n            bigquery.ScalarQueryParameter(\"end_date\", \"DATE\", end_date),\n        ]\n    )\n    query_job = client.query(query, job_config=job_config)\n    results = query_job.result().to_dataframe()\n    return results\n```\n## Usage\n```python\nsales_data = query_sales_by_date(\"2025-01-01\", \"2025-01-31\")\nprint(sales_data.head())\n```\n## Loading Data into BigQuery \n__Example:__ Uploading a CSV File\n\nIf you have a local CSV file (e.g., new_transactions.csv), you can load it into BigQuery:\n```python\ndef load_csv_to_bigquery():\n    table_id = \"your_project.your_dataset.new_transactions\"\n\n    job_config = bigquery.LoadJobConfig(\n        source_format=bigquery.SourceFormat.CSV,\n        skip_leading_rows=1,\n        autodetect=True,\n        write_disposition=\"WRITE_TRUNCATE\"\n    )\n\n    with open(\"new_transactions.csv\", \"rb\") as source_file:\n        job = client.load_table_from_file(\n            source_file, table_id, job_config=job_config\n        )\n\n    job.result()  # Wait for the job to complete\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_csv_to_bigquery()\n```\n## Best Practices:\n\nUse WRITE_TRUNCATE to replace the table or WRITE_APPEND to add data.\nFor large files, consider using Cloud Storage as an intermediate step.\n\n__Example:__ Loading from Pandas DataFrame\n\n```python\nimport pandas as pd\n\ndef load_dataframe_to_bigquery():\n    data = {\n        \"transaction_id\": [\"1001\", \"1002\", \"1003\"],\n        \"user_id\": [\"user1\", \"user2\", \"user3\"],\n        \"amount\": [99.99, 149.99, 199.99]\n    }\n    df = pd.DataFrame(data)\n    table_id = \"your_project.your_dataset.new_transactions_df\"\n\n    job = client.load_table_from_dataframe(\n        df, table_id, job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n    )\n    job.result()\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_dataframe_to_bigquery()\n```\n## ‚úç Writing Data to BigQuery \nExample: Streaming Real-Time Data\nIf you have real-time data (e.g., from an API), you can stream it into BigQuery:\n```python\ndef stream_real_time_data(rows_to_insert):\n    table_id = \"your_project.your_dataset.real_time_transactions\"\n    table = client.get_table(table_id)\n\n    errors = client.insert_rows(table, rows_to_insert)\n    if errors:\n        print(f\"Encountered errors: {errors}\")\n    else:\n        print(\"Data streamed successfully.\")\n\n# Example data\nrows_to_insert = [\n    {\"transaction_id\": \"1001\", \"user_id\": \"user1\", \"amount\": 99.99},\n    {\"transaction_id\": \"1002\", \"user_id\": \"user2\", \"amount\": 149.99}\n]\n\nstream_real_time_data(rows_to_insert)\n```\n### Note:\n\nStreaming is ideal for low-latency use cases but incurs higher costs.\nFor batch processing, use load_table_from_dataframe or load_table_from_file.\n\n\n## Scheduled Queries with Python\n__Example:__ Automating Daily Reports\nUse Cloud Scheduler and Cloud Functions to run queries on a schedule. Here‚Äôs a Python function for a Cloud Function:\n```python\ndef generate_daily_report(request):\n    client = bigquery.Client()\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) = CURRENT_DATE()\n        GROUP BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    # Send results via email or save to Cloud Storage\n    print(results)\n    return \"Report generated successfully.\"\n```\n## Deployment:\n\nDeploy this function to Cloud Functions and trigger it daily using Cloud Scheduler.\n\n\n## Optimizing Query Performance & Best Practices\n\nPartition your tables by date or integer ranges to reduce query costs.\nUse clustering for frequently filtered columns.\nAvoid SELECT *‚Äîonly query the columns you need.\nLeverage materialized views for repetitive queries.\n\n__Example:__  Creating a Partitioned Table\n```python\ndef create_partitioned_table():\n    table_id = \"your_project.your_dataset.partitioned_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"transaction_time\", \"TIMESTAMP\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.time_partitioning = bigquery.TimePartitioning(\n        type_=bigquery.TimePartitioningType.DAY,\n        field=\"transaction_time\"\n    )\n\n    table = client.create_table(table)\n    print(f\"Created partitioned table {table.table_id}\")\n\ncreate_partitioned_table()\n```\n__Example:__  Creating a Clustered Table\n```python\ndef create_clustered_table():\n    table_id = \"your_project.your_dataset.clustered_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"user_id\", \"STRING\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.clustering_fields = [\"user_id\"]\n\n    table = client.create_table(table)\n    print(f\"Created clustered table {table.table_id}\")\n\ncreate_clustered_table()\n```\n## Exporting Data from BigQuery \n#### __Example:__ Exporting Query Results to CSV\n```python\ndef export_to_csv():\n    query = \"\"\"\n        SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\n        WHERE transaction_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    results.to_csv(\"recent_transactions.csv\", index=False)\n    print(\"Data exported to recent_transactions.csv\")\n\nexport_to_csv()\nExample: Exporting to Cloud Storage\ndef export_to_cloud_storage():\n    destination_uri = \"gs://your-bucket/recent_transactions.avro\"\n    dataset_ref = client.dataset(\"your_dataset\", project=\"your_project\")\n    table_ref = dataset_ref.table(\"ecommerce_transactions\")\n\n    extract_job = client.extract_table(\n        table_ref,\n        destination_uri,\n        location=\"US\"\n    )\n    extract_job.result()\n    print(f\"Exported data to {destination_uri}\")\n\nexport_to_cloud_storage()\n```\n## ‚õëÔ∏è Error Handling and Logging \nAlways include error handling to manage API limits, network issues, and invalid queries:\n```python\nfrom google.api_core.exceptions import GoogleAPICallError, RetryError\n\ndef safe_query(query):\n    try:\n        query_job = client.query(query)\n        return query_job.result()\n    except GoogleAPICallError as e:\n        print(f\"API Error: {e}\")\n    except RetryError as e:\n        print(f\"Retry Error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n```\n## Cost Management \n\nMonitor usage in the BigQuery UI under Query History.\nSet up alerts for unusual spending in Cloud Billing.\nUse flat-rate pricing for predictable workloads.\nOptimize queries to reduce data scanned.\n\n\n## Advanced Use Cases \n#### __Example:__ Using BigQuery ML\n```python\ndef create_ml_model():\n    query = \"\"\"\n        CREATE OR REPLACE MODEL `your_project.your_dataset.sales_forecast_model`\n        OPTIONS(\n            model_type=ARIMA\n            time_series_timestamp_col=transaction_date\n            time_series_data_col=total_sales\n        ) AS\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n    \"\"\"\n    client.query(query).result()\n    print(\"ML model created successfully.\")\n\ncreate_ml_model()\n```\nExample: Integrating with Dataflow\n# Example Apache Beam pipeline for Dataflow\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef run_dataflow_pipeline():\n    options = PipelineOptions(\n        project=\"your_project\",\n        runner=\"DataflowRunner\",\n        region=\"us-central1\"\n    )\n\n    with beam.Pipeline(options=options) as p:\n        (p\n         | \"Read from BigQuery\" >> beam.io.ReadFromBigQuery(\n             query=\"SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\",\n             use_standard_sql=True\n         )\n         | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n             table=\"your_project.your_dataset.processed_transactions\",\n             schema=\"transaction_id\\:STRING, user_id\\:STRING, amount\\:FLOAT64\",\n             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n         )\n        )\n\nrun_dataflow_pipeline()\n```\n\n## Integrating with Other GCP Services \n\n### __Example:__ \n#### Triggering BigQuery from Cloud Storage\n```python\nfrom google.cloud import storage\n\ndef trigger_bigquery_on_new_file(bucket_name, file_name):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(file_name)\n\n    if blob.exists():\n        load_csv_to_bigquery(f\"gs://{bucket_name}/{file_name}\")\n    else:\n        print(f\"File {file_name} not found in bucket {bucket_name}\")\n\ntrigger_bigquery_on_new_file(\"your-bucket\", \"new_transactions.csv\")\n```\n## üîê Security Best Practices \n- Use IAM roles to grant least privilege access.\n- Encrypt sensitive data using Cloud KMS.\n- Audit logs to monitor access and changes.\n\n\n## Conclusion \nGoogle BigQuery and Python are a powerful combination for data analysis, ETL, and real-time processing. By following the examples and best practices above, you can start building scalable, efficient data pipelines on GCP.",
    "summary": "This guide provides real-world code examples for integrating Google BigQuery with Python on GCP.",
    "read_time": "18 min read",
    "tags": "gcp,google,python",
    "category": "gcp",
    "created_on": "2025-08-24 03:55:33",
    "updated_on": "2025-08-24 03:55:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "24",
    "slug": "go-fiber-ref",
    "title": "Go Fiber HTTP Methods",
    "subtitle": "A list of HTTP methods for the Fiber API",
    "author": "Keith Thomson",
    "content": "# Fiber Go üêÆ Request Methods\n\n#### These are how you access client request data.\n\n### üöÄGeneral Request Info\n\n- ctx.Method() []byte ‚Üí HTTP method (GET, POST, etc.)\n- ctx.Path() []byte ‚Üí Request path (e.g. /users/123)\n- ctx.RequestURI() []byte ‚Üí Full request URI\n- ctx.Host() []byte ‚Üí Host header\n- ctx.RemoteAddr() net.Addr ‚Üí Client IP + port\n- ctx.RemoteIP() net.IP ‚Üí Just the client IP\n\n### ‚≠êQuery & Params\n\n- ctx.QueryArgs() *fasthttp.Args ‚Üí All query string args (?foo=bar)\n- ctx.QueryArgs().Peek(\"foo\") ‚Üí []byte value for foo\n- ctx.UserValue(\"param\") ‚Üí Path parameter (if you use a router like fasthttprouter)\n\n### ‚≠êHeaders\n \n- ctx.Request.Header (struct with methods):\n- .Peek(\"Header-Name\") []byte\n- .ContentType() []byte\n- .UserAgent() []byte\n- .Referer() []byte\n- .Cookie(\"name\") []byte\n\n### ‚≠êBody\n\n- ctx.PostBody() []byte ‚Üí Raw body\n- ctx.FormValue(\"key\") []byte ‚Üí Form field (works with application/x-www-form-urlencoded)\n- ctx.MultipartForm() (*multipart.Form, error) ‚Üí File uploads / multipart form\n- ctx.IsBodyStream() ‚Üí Whether body is a streaming input\n\n## Response ‚Äî w methods\n\n#### These control what gets sent back.\n\n### üòÅHeaders & Status\n\n- ctx.SetStatusCode(code int) ‚Üí Set status (200, 404, etc.)\n- ctx.Response.Header.Set(\"Header\", \"value\")\n- ctx.Response.Header.SetContentType(\"application/json\")\n- ctx.Response.Header.SetCanonical([]byte(\"X-Header\"), []byte(\"val\"))\n- ctx.Response.Header.SetCookie(cookie *fasthttp.Cookie)\n\n## üêº Body Writing\n\n- ctx.SetBody([]byte)\n- ctx.SetBodyString(string)\n- ctx.SetBodyStream(r io.Reader, size int)\n- ctx.Write([]byte) ‚Üí Writes to response (append-style)\n- ctx.WriteString(string)\n- ctx.SendFile(\"path/to/file\") ‚Üí Serve static files\n\n### üëâ Redirects\n\n- ctx.Redirect(uri string, statusCode int)\n- ctx.RedirectBytes(uri []byte, statusCode int)\n\n### ‚öôÔ∏è Utility & Middleware Helpers\n\n- ctx.Next() ‚Üí (if using middleware chains, e.g. in fasthttprouter)\n- ctx.Done() ‚Üí Context cancellation\n- ctx.Time() ‚Üí Request start timestamp\n- ctx.Response.Reset() ‚Üí Clear response\n- ctx.Request.Reset() ‚Üí Clear request\n\n### ‚úÖ Quick Example\n\n##### Simple API using Go with Fiber\n```go\nfunc main() {\n  app := fiber.New()\n\n  api := app.Group(\"/api\", middleware) // /api\n\n  v1 := api.Group(\"/v1\", middleware)   // /api/v1\n  v1.Get(\"/list\", handler)             // /api/v1/list\n  v1.Get(\"/user\", handler)             // /api/v1/user\n\n  v2 := api.Group(\"/v2\", middleware)   // /api/v2\n  v2.Get(\"/list\", handler)             // /api/v2/list\n  v2.Get(\"/user\", handler)             // /api/v2/user\n\n  log.Fatal(app.Listen(\":3000\"))\n}\n```",
    "summary": "A list of Methods from the Fiber API",
    "read_time": "2 min read",
    "tags": "fiber,go,http",
    "category": "golang,fiber,http",
    "created_on": "2025-09-21 08:33:31",
    "updated_on": "2025-09-21 08:33:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "25",
    "slug": "go-http-requests",
    "title": "Understanding HTTP Requests in Go",
    "subtitle": "A Guide to Handling HTTP in Go with net/http and Fiber",
    "author": "Keith Thomson",
    "content": "<img src=\"/static/assets/icons/cncf.png\" width=\"200\" height=\"100\">\n\nThink of an HTTP request like mailing a package. The different parts of the request correspond to different parts of the mailing process:\n\n*   **URL Path** (`/users/123`): The street address on the package.\n*   **HTTP Method** (`GET`, `POST`): The delivery instruction (e.g., \"Standard Delivery\" or \"Signature Required\").\n*   **Headers**: The labels on the outside of the box (From, To, Contents: Books, Fragile: Yes).\n*   **Body**: The actual items inside the box (like a JSON payload).\n\n## ü´ÄThe Anatomy of an HTTP Request\n\nA raw request sent from a client looks something like this:\n\n```http\nGET /posts/42?comments=true HTTP/1.1\nHost: whalerapi.com\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64)\nAccept: application/json\nAuthorization: Bearer my-secret-auth-token\n\n{\n  \"key\": \"This is the optional request body, often used with POST or PUT\"\n}\n```\n\nLet's break down how Go gives you access to this information.\n\n## How Go Parses the Request üìù\n\nWhether you use Go's standard library or a framework like Fiber, the request is parsed into an object that you can easily query in your handler.\n\n### Using the Standard `net/http` Library\n\nIn a standard Go handler, you get a pointer to an http.Request object (conventionally named r).\n\n```go\nfunc myHandler(w http.ResponseWriter, r *http.Request) {\n    // 1. Get the Method and Path\n    method := r.Method           // \"GET\"\n    path := r.URL.Path           // \"/posts/42\"\n\n    // 2. Get Query Parameters\n    showComments := r.URL.Query().Get(\"comments\") // \"true\"\n\n    // 3. Get Headers\n    // Use .Get() to read a specific header's value\n    userAgent := r.Header.Get(\"User-Agent\")      // \"Mozilla/5.0...\"\n    authToken := r.Header.Get(\"Authorization\")   // \"Bearer my-secret-auth-token\"\n    \n    // You can also iterate through all headers\n    for name, values := range r.Header {\n        // values is a slice of strings\n        fmt.Printf(\"Header '%s': %s\\n\", name, values[0])\n    }\n\n    // 4. Read the Body (important for POST, PUT, PATCH)\n    body, err := io.ReadAll(r.Body)\n    if err != nil {\n        // Handle error\n    }\n    // Now 'body' is a byte slice containing the JSON\n}\n```\n\n### Using the Fiber Framework\n\nIn Fiber, you get a context object (`*fiber.Ctx`, conventionally named `c`), which provides convenient helper methods.\n\n```go\nfunc myFiberHandler(c *fiber.Ctx) error {\n    // 1. Get the Method and Path\n    method := c.Method() // \"GET\"\n    path := c.Path()     // \"/posts/42\"\n\n    // 2. Get Query Parameters\n    showComments := c.Query(\"comments\") // \"true\"\n\n    // 3. Get Headers\n    // Use .Get() which is a convenient wrapper\n    userAgent := c.Get(\"User-Agent\")    // \"Mozilla/5.0...\"\n    authToken := c.Get(\"Authorization\") // \"Bearer my-secret-auth-token\"\n\n    // 4. Read the Body\n    body := c.Body() // Returns the body as a byte slice\n    \n    // Fiber also has helpers to parse the body directly into a struct\n    var requestData MyStruct\n    if err := c.BodyParser(&requestData); err != nil {\n        // Handle error\n    }\n\n    return c.SendString(\"Request parsed!\")\n}\n```\n\n## How to Use Headers to Create Better Code\n\nAccessing headers allows you to build more robust, secure, and flexible applications. Here are the most common use cases:\n\n### 1. `Content-Type`: Handling Different Data Formats\n\nA client uses the `Content-Type` header to tell you what format the request body is in (e.g., `application/json`, `application/x-www-form-urlencoded`). You can check this header to decide how to parse the incoming data.\n\n```go\nfunc createUser(c *fiber.Ctx) error {\n    var user User\n\n    // Check what kind of data the client sent\n    if c.Is(\"json\") {\n        // If it's JSON, parse it from the body\n        if err := c.BodyParser(&user); err != nil {\n            return c.Status(fiber.StatusBadRequest).SendString(\"Invalid JSON\")\n        }\n    } else {\n        return c.Status(fiber.StatusUnsupportedMediaType).SendString(\"Content-Type must be application/json\")\n    }\n\n    // ... save the user to the database ...\n    return c.Status(fiber.StatusCreated).JSON(user)\n}\n```\n\n### 2. `Authorization`: Securing Your Endpoints\n\nThis is the most common header for API security. A client sends a token (like a JWT or an API key), and you can write middleware to verify it before allowing the request to proceed.\n\n```go\nimport \"strings\"\n\nfunc AuthMiddleware(c *fiber.Ctx) error {\n    // Get the Authorization header\n    authHeader := c.Get(\"Authorization\")\n\n    // Check if it's missing or doesn't start with \"Bearer \"\n    if authHeader == \"\" || !strings.HasPrefix(authHeader, \"Bearer \") {\n        return c.Status(fiber.StatusUnauthorized).SendString(\"Missing or malformed JWT\")\n    }\n\n    // Get the token itself (everything after \"Bearer \")\n    tokenString := strings.TrimPrefix(authHeader, \"Bearer \")\n\n    // Here, you would validate the token (e.g., jwt.Parse())\n    if !isValid(tokenString) {\n        return c.Status(fiber.StatusUnauthorized).SendString(\"Invalid JWT\")\n    }\n\n    // If valid, proceed to the next handler in the chain\n    return c.Next()\n}\n\n// You would apply this in your main app setup:\n// app.Use(AuthMiddleware)\n```\n### 3. `Accept`: Content Negotiation\n\nThe `Accept` header is how a client tells you what data format it wants back. For example, it might request `application/json` or `application/xml`. Your server can check this header and format the response accordingly.\n\n```go\nfunc getUser(c *fiber.Ctx) error {\n    user := GetUserFromDB() // Get user data\n\n    // Check what format the client wants\n    switch c.Accepts(\"json\", \"xml\") {\n    case \"json\":\n        return c.JSON(user)\n    case \"xml\":\n        return c.XML(user)\n    default:\n        // Default to JSON if the client doesn't specify\n        return c.JSON(user)\n    }\n}\n```",
    "summary": "A guide to handling HTTP requests in Go using the Fiber framework.",
    "read_time": "6 min read",
    "tags": "fiber,http,go",
    "category": "http",
    "created_on": "2025-09-22 10:00:00",
    "updated_on": "2025-09-22 10:00:00",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "27",
    "slug": "search-go-htmx",
    "title": "Go and HTMX",
    "subtitle": "Implementing Search Functionality",
    "author": "Keith Thomson",
    "content": "![](https://preview.redd.it/tcmyd3n69ng41.jpg?width=1999&format=pjpg&auto=webp&s=b79cf22d3e2adcaf52a2d22bcb0568e42eff8bc2) \n\n ### üî¨Implementing Search with HTMX\n\nThis guide provides the code to implement a search functionality for your blog using HTMX.\n\n## üß≠ Search Logic\n\nCreate a new file `services/search_posts.go` and add the following code. This file will contain the logic for searching posts.\n\n```go\npackage services\n\nimport (\n\n\t\"encoding/json\"\n\n\t\"io\"\n\n\t\"os\"\n\n\t\"strings\"\n\n\t\"github.com/whaler/templ-go/models\"\n\n)\n\nfunc SearchPosts(query string) ([]models.Post, error) {\n\n\tfile, err := os.Open(\"allposts.json\")\n\n\tif err != nil {\n\n\t\treturn nil, err\n\n\t}\n\n\tdefer file.Close()\n\n\tbytes, err := io.ReadAll(file)\n\n\tif err != nil {\n\n\t\treturn nil, err\n\n\t}\n\n\tvar posts []models.Post\n\n\tif err := json.Unmarshal(bytes, &posts); err != nil {\n\n\t\treturn nil, err\n\n\t}\n\n\tif query == \"\" {\n\n\t\treturn posts, nil\n\n\t}\n\n\tvar filteredPosts []models.Post\n\n\tfor _, post := range posts {\n\n\t\tif strings.Contains(strings.ToLower(post.Title), strings.ToLower(query)) ||\n\n\t\t\tstrings.Contains(strings.ToLower(post.Subtitle), strings.ToLower(query)) ||\n\n\t\t\tstrings.Contains(strings.ToLower(post.Summary), strings.ToLower(query)) {\n\n\t\t\tfilteredPosts = append(filteredPosts, post)\n\n\t\t}\n\n\t}\n\n\treturn filteredPosts, nil\n\n}\n```\n\n## üîç Search Handler\n\nCreate a new file `handlers/handle_search.go` and add the following code. This handler will process the search requests.\n\n```go\npackage handlers\n\nimport (\n\n\t\"net/http\"\n\n\t\"github.com/whaler/templ-go/services\"\n\n\t\"github.com/whaler/templ-go/templates\"\n\n)\n\nfunc HandleSearchPosts(w http.ResponseWriter, r *http.Request) {\n\n\tquery := r.FormValue(\"query\")\n\n\tposts, err := services.SearchPosts(query)\n\n\tif err != nil {\n\n\t\thttp.Error(w, \"Failed to search posts\", http.StatusInternalServerError)\n\n\t\treturn\n\n\t}\n\n\t// We are only rendering the post cards, not the whole page\n\n\tcomponent := templates.PostCardList(posts)\n\n\tcomponent.Render(r.Context(), w)\n\n}\n```\n\n## üó∫Ô∏è Add the Search Route\n\nModify your `main.go` file to add the new `/search` route. You will need to convert your app to use `net/http` handlers, or use an adapter. Given you are using `fiber`, I will show how to adapt the `net/http` handler.\n\nFirst, you need to get the adaptor by running this command:\n\n`go get github.com/gofiber/adaptor/v2`\n\nThen, in your `main` function, add the following route:\n\n```go\napp.Post(\"/search\", adaptor.HTTPHandlerFunc(handlers.HandleSearchPosts))\n```\n\nSo your `main.go` will look something like this (showing only the relevant part):\n\n```go\npackage main\n\nimport (\n\n\t\"log\"\n\n\t\"sort\"\n\n\t\"github.com/gofiber/fiber/v2\"\n\n\t\"github.com/gofiber/adaptor/v2\"\n\n\t\"github.com/whalerapi/templ-go/handlers\"\n\n\t\"github.com/whalerapi/templ-go/models\"\n\n\t\"github.com/whalerapi/templ-go/services\"\n\n\t\"github.com/whalerapi/templ-go/templates\"\n\n)\n\n// ... (rest of your main.go file)\n\nfunc main() {\n\n\t// ... (your existing code)\n\n\tapp.Get(\"/about\", handlers.AboutHandler)\n\n\tapp.Get(\"/all_posts_page\", handlers.HandleAllPosts)\n\n\tapp.Get(\"/blog/:slug\", handlers.HandlePost)\n\n\tapp.Get(\"/resume\", handlers.ResumeHandler(models.MyProfile))\n\n\tapp.Post(\"/search\", adaptor.HTTPHandlerFunc(handlers.HandleSearchPosts)) // Add this line\n\n\tlog.Println(\"Listening on :8080\")\n\n\tif err := app.Listen(\":8080\"); err != nil {\n\n\t\tlog.Fatalf(\"failed to start server: %v\", err)\n\n\t}\n\n}\n```\n\n## 4. Update the Frontend\n\nModify your `templates/all_posts_page.templ` file to add the search bar with HTMX attributes.\n\nReplace this:\n\n```html\n<div class=\"mb-8\">\n\n\t<input type=\"text\" placeholder=\"Search posts...\" class=\"border border-gray-300 rounded-lg py-2 px-4 w-full\"/>\n\n</div>\n```\n\nWith this:\n\n```html\n<div class=\"mb-8\">\n\n\t<input type=\"text\" name=\"query\" placeholder=\"Search posts...\"\n\n\t\tclass=\"border border-gray-300 rounded-lg py-2 px-4 w-full\"\n\n\t\thx-post=\"/search\"\n\n\t\thx-trigger=\"keyup changed delay:500ms\"\n\n\t\thx-target=\"#search-results\"\n\n\t\thx-swap=\"innerHTML\"\n\n\t\thx-indicator=\".htmx-indicator\"\n\n\t/>\n\n\t<span class=\"htmx-indicator\">Searching...</span>\n\n</div>\n```\n\nThen, wrap the grid of posts in a `div` with the `id=\"search-results\"`.\n\nReplace this:\n\n```html\n<div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n\n\tfor _, post := range posts {\n\n\t\t@PostCard(post)\n\n\t}\n\n</div>\n```\n\nWith this:\n\n```html\n<div id=\"search-results\" class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n\n\tfor _, post := range posts {\n\n\t\t@PostCard(post)\n\n\t}\n\n</div>\n```\n\nFinally, you need to make sure you have HTMX included in your project. You can add this to your main layout file (`templates/layout.templ`) in the `<head>` section:\n\n```html\n<script src=\"https://unpkg.com/htmx.org@1.9.10\"></script>\n```\n\nThat's it! After making these changes, you will have a working search functionality on your \"All Posts\" page.",
    "summary": "This post details how to implement a search function and dynamically search through a .json file. ",
    "read_time": "12 min read",
    "tags": "go,json",
    "category": "go,automation,webdev",
    "created_on": "2025-10-04 22:32:07",
    "updated_on": "2025-10-04 22:32:07",
    "published": "1",
    "featured": "1"
    },
    {
    "id": "28",
    "slug": "simulations-in-js",
    "title": "Using P5.js and Matter.js for Simuations",
    "subtitle": "Replicate real world physics with this powerful libary",
    "author": "Keith Thomson",
    "content": "### Building a Simple Pendulum Simulation with p5.js and matter.js \n\nThis guide will show you how to create a physics-based pendulum simulation using the p5.js and matter.js frameworks. This project is simple enough to understand quickly but demonstrates physics interactions and rendering, making it meaningful for learning.\n\n## 1. Setting Up Your Project\n\nFirst, create a new folder for your project and add an `index.html` file. Include the libraries via CDN links.\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Pendulum Simulation</title>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.6.0/p5.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/matter-js/0.20.1/matter.min.js\"></script>\n</head>\n<body>\n    <script src=\"sketch.js\"></script>\n</body>\n</html>\n```\n\nCreate a new `sketch.js` file in the same folder. This is where we will write our simulation code.\n\n## 2. Initializing matter.js in p5.js\n\nWe need to create the engine, world, and bodies. We'll create a pendulum with a bob and a fixed point.\n\n```javascript\n// module aliases\nconst Engine = Matter.Engine,\n      World = Matter.World,\n      Bodies = Matter.Bodies,\n      Constraint = Matter.Constraint;\n\nlet engine;\nlet world;\nlet pendulumBob;\nlet pendulumConstraint;\n\nfunction setup() {\n    createCanvas(600, 400);\n\n    // create engine and world\n    engine = Engine.create();\n    world = engine.world;\n\n    // create pendulum bob\n    pendulumBob = Bodies.circle(300, 200, 20, { restitution: 0.9, density: 0.01 });\n    World.add(world, pendulumBob);\n\n    // create constraint (string)\n    let options = {\n        pointA: { x: 300, y: 100 },\n        bodyB: pendulumBob,\n        length: 150,\n        stiffness: 1\n    };\n\n    pendulumConstraint = Constraint.create(options);\n    World.add(world, pendulumConstraint);\n}\n\nfunction draw() {\n    background(220);\n    Engine.update(engine);\n\n    // draw the pendulum string\n    stroke(0);\n    strokeWeight(2);\n    line(pendulumConstraint.pointA.x, pendulumConstraint.pointA.y, pendulumBob.position.x, pendulumBob.position.y);\n\n    // draw the bob\n    fill(127);\n    ellipse(pendulumBob.position.x, pendulumBob.position.y, 40);\n}\n```\n\n## 3. Adding Interactivity\n\nLet's allow the user to click and drag the pendulum bob.\n\n```javascript\nfunction mouseDragged() {\n    Matter.Body.setPosition(pendulumBob, { x: mouseX, y: mouseY });\n}\n```\n\nThis simple addition allows users to interact with the simulation and see realistic pendulum motion based on physics.\n\n## 4. How It Works\n\n- **Engine & World:** matter.js uses these to simulate physics.\n- **Bodies:** The bob is a circle body with some restitution and density.\n- **Constraint:** Acts like a string connecting the fixed point to the bob.\n- **p5.js Rendering:** We draw the bob and line each frame using `draw()`.\n- **Interactivity:** `mouseDragged()` updates the bob‚Äôs position, and physics handles the resulting motion.\n\n## 5. Next Steps / Ideas\n\n- Add multiple pendulums to create a simple Newton's cradle.\n- Introduce obstacles or boundaries for collision testing.\n- Implement gravity changes and damping for more realistic motion.\n- Add color coding or trails to visualize the path of the pendulum.\n\nThis simulation demonstrates the power of combining **p5.js** for rendering and **matter.js** for physics, creating interactive and visually appealing experiments with minimal code.\n\nNow, simply open your `index.html` in a browser to see the pendulum in action!",
    "summary": "In this guide we built a simple pendulum with p5.js and matter.js",
    "read_time": "15 min read",
    "tags": "js,webdev,javascript",
    "category": "javascript,js",
    "created_on": "2025-10-04 22:32:07",
    "updated_on": "2025-10-04 22:32:07",
    "published": "1",
    "featured": "1"
  }
]
